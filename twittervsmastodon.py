# -*- coding: utf-8 -*-
"""TwittervsMastodon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vDfTM1wyN-7bu5DlhcPLV5cYreMDaRKX

# Is Mastodon Actually Better than Twitter? - A Closer Look at Privacy Policies 

## Introduction

In January of 2022, Elon Musk started [showing interest in owning Twitter](https://apnews.com/article/twitter-elon-musk-timeline-c6b09620ee0905e59df9325ed042a609) by purchasing a small percentage of its shares. In less than a year on October 27th, 2022, Elon Musk closed a $44 billion deal to purchase Twitter and officially took over one of the major social media platforms in the world. With Musk’s mixed reputation in the technology space as well as [his vision](https://www.cnn.com/2022/10/28/tech/elon-musk-twitter-changes/index.html#:~:text=Musk%20has%20also%20said%20he,or%20demoted%20in%20their%20feed. ) for the platform, his leadership alarmed many users as Elon Musk has heavily focused on profit and free speech over user’s trust and safety. In search of an alternative, users sought out a similar platform called Mastodon. [Mastodon](https://www.nytimes.com/2022/11/07/technology/mastodon-twitter-elon-musk.html ) is a social media platform that was founded in 2016 by the software developer Eugen Rochko. Mastodon claims that it can be a [“viable alternative to Twitter”](https://blog.joinmastodon.org/2022/04/twitter-buyout-puts-mastodon-into-spotlight/ ) as a free and open source platform where communication is not dictated by one commercial company. Its users have [grown significantly](https://www.cnn.com/2022/12/20/tech/mastodon-twitter-usage/index.html) since Musk's takeover of Twitter.

Elon Musk’s leadership has raised many questions about how social media should be governed and operated. Among many issues that social media companies are scrutinized for, privacy and data are one of the major topics discussed. There have been many data breaches of social media companies including a major revelation called the [Cambridge Analytica Scandal](https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html) in 2018, where millions of Facebook user’s data were breached and used as a political tool. Considering the significance of privacy and data, this paper aims to compare the privacy policies of Twitter and its alternative, Mastodon. 

The hypothesis for this paper is that Matsudon is a better social media platform than Twitter, therefore Mastodon must have a good if not a better privacy policy than Twitter. In this context, a good privacy policy means setting out, at minimum, answers to these questions building upon [UC Berkeley's guidelines](https://security.berkeley.edu/how-write-effective-website-privacy-statement#:~:text=Your%20privacy%20statement%20must%20accurately,you%20must%20inform%20your%20users): what kind of data are collected, what is the legal basis of collecting such data, how are these data are used, who to contact regarding policies, how long are the data retained, what measures are taken to protect and secure the data, and who has access to the data. The goal is to compare the two policies to determine which platform has the most comprehensive policy that seeks to provide all the necessary information to the users but also do due diligence to protect the data. Vague language and difficult and technical jargons used in the policies will be considered against the platforms’ policies.

For the purpose of this paper, technical jargon is limited to the use of abbreviations. For instance, if the policy states GDPR, a European regulation on data instead of writing out General Data Protection Regulation, it will be considered a technical jargon. In terms of vague language, it will be limited to ambiguity coming from use of language that does not give certainty, such as ‘may’ will be considered. 

## Methodology

Data collection for this method is fairly simple. The privacy policies are to be collected by downloading [Twitter](https://twitter.com/en/privacy) and [Mastodon](https://mastodon.social/privacy-policy)’s policies from their websites and by using PyMuPDF, the texts will be extracted. 

First part of the research will go through the questions asked to be answered in privacy policies through search using python using readlines function. Then, technical jargons will be searched using the Spacy library. Data will be then cleaned up using NLTK.  

Considering Twitter’s privacy policy is longer and more detailed, using lexical density, a quantitative way of finding out breadth of vocabularies, the research seeks to answer which policy is more robust. Additionally, the term frequency-inverse document frequency (TF-IDF) method will be used. Using this method, the most frequent and significant terms used in each company’s policies will be identified. Lastly, some of the visualization tools would also be used. For instance, Word Cloud will show the common word and possibly show in sizes the relative frequency of the term mentioned in each of the policies as well as a heat map showing the top 10 TF-IDF terms for each policy. 

Unless otherwise specified in this document, all codes used in this document are from the [Introduction to Text Analysis](https://github.com/intro-to-text-analysis-SIPA-S23/syllabus) course taught by Rebecca Krisel.

## Data Scraping

First scrape the policy from PDF using PyMuPDF

Install PyMuPDF library
"""

!pip install PyMuPDF

"""Extract the text from the PDF files and create a text document"""

import fitz #fitz is pymupdf
doc = fitz.open("Twitter_Privacy_Policy_EN.pdf")  # open document
out = open("twitter.txt", "wb")  # open text output
for page in doc:  # iterate the document pages
    text = page.get_text().encode("utf8")  # get plain text (is in UTF-8)
    out.write(text)  # write text of page
    out.write(bytes((12,)))  # write page delimiter (form feed 0x0C)
out.close()

doc2 = fitz.open("Mastodon_Privacy_Policy.pdf")
out2 = open("mastodon.txt", "wb") 
for page2 in doc2:  # iterate the document pages
    text2 = page2.get_text().encode("utf8")  # get plain text (is in UTF-8)
    out2.write(text2)  # write text of page
    out2.write(bytes((12,)))  # write page delimiter (form feed 0x0C)
out2.close()

"""## Answering UK Berkley's Guidelines using readlines function. 

The following script is adopted from [GeeksforGeeks](https://www.geeksforgeeks.org/python-how-to-search-for-a-string-in-text-files/# )

First question is: What kind of data are collected? For this question, data and information are used as search terms.
"""

word = 'data'
with open(r'twitter.txt', 'r') as twitter:
    # read all lines in a list
    lines = twitter.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""Looking at the document from line 56, Twitter provides very detailed information about the data that are collected including email address or phone number, date of birth, as well as information collected as a user, such as content that you post, interaction with other contents, links that you interact with. Additionally, Twitter collects device and location information, even the interactions with Twitter content on third-party sites."""

word = 'information'
with open(r'mastodon.txt', 'r') as mastodon:
    # read all lines in a list
    lines = mastodon.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""From line 12 of Mastodon document, it states that username, e- mail address and password, contents, log information, etc are colleted. 

Mastodon seems to collect less information from users. This may be because the products that Twitter offers is more complex than Mastodon. 

Ultimately, both policies meet the minimum requirement of answering what data are collected.

Second question is: What is the legal basis of collecting such data? For this question, law and legal are used as search terms.
"""

word = 'legal'
with open(r'twitter.txt', 'r') as twitter:
    # read all lines in a list
    lines = twitter.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""In Line 407, section 6.1 of the document, Twitter mentions that it has "carefully considered the legal reasons it is permitted to collect,
use, share and otherwise process your information." There is no specific mention of the laws and regulation themselves. It does not fully answer the question but it does mention the legal basis. 
"""

word = 'law'
with open(r'mastodon.txt', 'r') as mastodon:
    # read all lines in a list
    lines = mastodon.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""Law is mentioned in Mastodon's policy, however, it also does not mention the legal basis on collection of the data. The statement is on sharing the data. 

For question 2, both Twitter and Mastodon do not cover the legal basis.

Third question is: How are data used? For this question, 'use' is used as a search term.
"""

word = 'use'
with open(r'twitter.txt', 'r') as twitter:
    # read all lines in a list
    lines = twitter.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""There are many mentions of 'use' in Twitter's document. Section provided on Line 2, Twitter provides comprehensive explanation on where the collected data are used, such as for personalized service, service analysis, and safety of users."""

word = 'use'
with open(r'mastodon.txt', 'r') as mastodon:
    # read all lines in a list
    lines = mastodon.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""For Mastodon, on Line 42: “What do we use your information for?”, the document provides explanations of where the data are used, such as for core functionality, moderation of the community, and others.

Twitter and Mastodon both answer the third question.

Fourth question is: Who to contact regarding policies? For this question, 'contact' is used as a search term.
"""

word = 'contact'
with open(r'twitter.txt', 'r') as twitter:
    # read all lines in a list
    lines = twitter.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""On Line 480, there is a section on how to contact Twitter. The document provides contact information based on where the user is located as well as a link to a web form. """

word = 'contact'
with open(r'mastodon.txt', 'r') as mastodon:
    # read all lines in a list
    lines = mastodon.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""Mastodon does not provide contact information for inquiries related to the privacy policy.

For question 4, only Twitter fulfills the requirement.

Fifth question is: How long are the data retained? For this question, 'period' and 'rentention' are used as search terms.
"""

word = 'period'
with open(r'twitter.txt', 'r') as twitter:
    # read all lines in a list
    lines = twitter.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""Twitter does provide the data retention period in line 335. This section provides how different types of information are retained for different periods. 

"""

word = 'retention'
with open(r'mastodon.txt', 'r') as mastodon:
    # read all lines in a list
    lines = mastodon.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""Mastodon also provides the data retention policy in line 50. The document provides retention period for server logs and IP address. 

Both Twitter and Mastodon fulfill the fifth requirement.

Sixth question is: What measures are taken to protect and secure the data? 'protect'is used as a search term.
"""

word = 'protect'
with open(r'twitter.txt', 'r') as twitter:
    # read all lines in a list
    lines = twitter.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""Although line 371 mention protecting and securing privacy, there is no specific mention on how. Same for line 432, when data is shared with third parties, Twitter requires them to protect the data but no mention of how. """

word = 'protect'
with open(r'mastodon.txt', 'r') as mastodon:
    # read all lines in a list
    lines = mastodon.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""Line 44 provides the section on how Mastodon protect user's information. 

This section mentions security measures such as hashing.

Only Mastodon fulfills the 6th requirement.

Last question is: Who has access to the data? 'share' and 'transfer' are used as search terms.
"""

word = 'share'
with open(r'twitter.txt', 'r') as twitter:
    # read all lines in a list
    lines = twitter.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

word = 'transfer'
with open(r'twitter.txt', 'r') as twitter:
    # read all lines in a list
    lines = twitter.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""There are many mentions of sharing and transferring data in the Twitter document. Thus, Twitter fulfills the requirement. """

word = 'share'
with open(r'mastodon.txt', 'r') as mastodon:
    # read all lines in a list
    lines = mastodon.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

word = 'transfer'
with open(r'mastodon.txt', 'r') as mastodon:
    # read all lines in a list
    lines = mastodon.readlines()
    for line in lines:
        # check if string present on a current line
        if line.find(word) != -1:
            print(word, 'string exists in file')
            print('Line Number:', lines.index(line))
            print('Line:', line)

"""Line 76 provides that Mastodon do not "sell, trade or otherwise transfer to outside parties your personally identifiable information"

Both Twitter and Mastodon fulfill the requirement of mentioning who has access to the user data.

For the first part of the analysis, both Twitter and Mastodon lack clarity on certain questions and need improvement on their policy.

## Search for Technical Jargons

Prior to cleaning up the data, spacy library is going to be used to search abbreviations in the document and check whether abbreviations are written out to avoid confusion.

Spacy library will be used for this analysis. The folllwing script is adapted from [allenai Github](https://https://github.com/allenai/scispacy).

First, install scispacy
"""

pip install scispacy

pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz

"""Create variables for the twitter and mastodon text files."""

file = open("twitter.txt", 'r')
print(file)
content = file.read()

file2 = open("mastodon.txt", 'r')
print(file2)
content2 = file2.read()

"""Import spacy and using the "en_core_sci_sm" dictionary, look for abbreviations in the document and find written out terms/definition in the document.

First is Twitter document.
"""

import spacy
from scispacy.abbreviation import AbbreviationDetector

nlp = spacy.load("en_core_sci_sm")

# Add the abbreviation pipe to the spacy pipeline.
nlp.add_pipe("abbreviation_detector")

doc = nlp(content)

print("Abbreviation", "\t", "Definition")
for abrv in doc._.abbreviations:
	print(f"{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}")

"""Twitter has abbreviations but does not seem to show on the result. After doing some manual search on the file, there were references to Digital Advertising Alliance (DAA), EFTA (European Free Trade Association) and others. However, certain words, such as EFTA was not defined in the document."""

import spacy
from scispacy.abbreviation import AbbreviationDetector

nlp = spacy.load("en_core_sci_sm")

# Add the abbreviation pipe to the spacy pipeline.
nlp.add_pipe("abbreviation_detector")

doc = nlp(content2)

print("Abbreviation", "\t", "Definition")
for abrv in doc._.abbreviations:
	print(f"{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}")

"""Mastodon has two acronyms that were both defined in the document.

Twitter should be more mindful of writing out the abbreviations for users that may not be familiar with certain terms.

## Data Clean up
To continue the analysus, the text data will be cleaned up using NLTK pos tag, lemmetizer and stopwords. 

First import all the libraries.
"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk import pos_tag
from nltk.corpus import wordnet 
from nltk.stem.wordnet import WordNetLemmatizer
stops = stopwords.words('english')

import glob

"""Create a folder for the clean up files to go into."""

! mkdir files_cleaned

"""Define a function to map POS tag."""

def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

"""Clean up the files by changing all terms to lowercase, filtering through stop words, and lemmatizing the terms. Finally, save the result into a new text file."""

nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('punkt')
text_tokens = nltk.word_tokenize(content)
nltk_text = nltk.Text(text_tokens)
text_lower = [t.lower() for t in nltk_text if t.isalnum()]
text_stops = [t for t in text_lower if t not in stops]
text_clean = [WordNetLemmatizer().lemmatize(t, get_wordnet_pos(t)) for t in text_stops]

with open(f"files_cleaned/twitter_cleaned.txt", "w") as file:
    file.write(str(text_clean))

text_tokens2 = nltk.word_tokenize(content2)
nltk_text2 = nltk.Text(text_tokens2)
text_lower2 = [t.lower() for t in nltk_text2 if t.isalnum()]
text_stops2 = [t for t in text_lower2 if t not in stops]
text_clean2 = [WordNetLemmatizer().lemmatize(t, get_wordnet_pos(t)) for t in text_stops2]

with open(f"files_cleaned/mastodon_cleaned.txt", "w") as file:
    file.write(str(text_clean2))

"""Check to see if any more stop words need to be added. """

file = open(f"files_cleaned/twitter_cleaned.txt", 'r')
twittercontent = file.read()
print(twittercontent)

file2 = open(f"files_cleaned/mastodon_cleaned.txt", 'r')
mastodoncontent = file2.read()
print(mastodoncontent)

"""To check for more significant words to be added to the stopwords list, split function and most_common functions are used to list the terms by most frequent terms in the document. 

The following script is adopted from [Geeks for Geeks.](https://www.geeksforgeeks.org/find-k-frequent-words-data-set-python/#) 
"""

from collections import Counter
  
# split() returns list of all the words in the string
split_it = twittercontent.split()
  
# Pass the split_it list to instance of Counter class.
Counter0 = Counter(split_it)

# most_common() produces k frequently encountered
# input values and their respective counts.
most_occur_twitter = Counter0.most_common(300)

    
print(most_occur_twitter)

split_it2 = mastodoncontent.split()
Counter2 = Counter(split_it2)
most_occur_mastodon = Counter2.most_common(300)
print(most_occur_mastodon)

"""From here, unnecessary words are identified and will be added to the stopwords list. 

The folllowing scrip is adopted from [Projectpro](https://www.projectpro.io/recipes/add-custom-stopwords-and-then-remove-them-from-text)

Add new stopwords, such as twitter, mastodon, privacy, policy, etc and add it to the existing stopwords.
"""

new_stopwords = ["twitter", 'mastodon', 'u', "also", "daily", "privacy", "policy", "http", 'last', 'update', 'oct', '06', '2022', ]
stops.extend(new_stopwords)

print(stops)

newstops = stops
print(newstops)

"""Re-run the data clean ups with new stopwords and override the current cleaned text files. """

text_tokens = nltk.word_tokenize(content)
nltk_text = nltk.Text(text_tokens)
text_lower = [t.lower() for t in nltk_text if t.isalnum()]
text_stops = [t for t in text_lower if t not in newstops]
text_clean = [WordNetLemmatizer().lemmatize(t, get_wordnet_pos(t)) for t in text_stops]

with open(f"files_cleaned/twitter_cleaned.txt", "w") as file:
    file.write(str(text_clean))

text_tokens4 = nltk.word_tokenize(content2)
nltk_text4 = nltk.Text(text_tokens4)
text_lower4 = [t.lower() for t in nltk_text4 if t.isalnum()]
text_stops4 = [t for t in text_lower4 if t not in newstops]
text_clean4 = [WordNetLemmatizer().lemmatize(t, get_wordnet_pos(t)) for t in text_stops4]

with open(f"files_cleaned/mastodon_cleaned.txt", "w") as file:
    file.write(str(text_clean4))

"""Open the file to check if the stopwords are reflected properly."""

file = open(f"files_cleaned/twitter_cleaned.txt", 'r')
newtwittercontent = file.read()
print(newtwittercontent)

file2 = open(f"files_cleaned/mastodon_cleaned.txt", 'r')
newmastodoncontent = file2.read()
print(newmastodoncontent)

"""Since 'http' was not deleted properly, by using replace function, http will be replaced with a space.

The following script is adopted from [Geeks for Geeks](https://www.geeksforgeeks.org/how-to-search-and-replace-text-in-a-file-in-python/#).
"""

with open(f"files_cleaned/mastodon_cleaned.txt", 'r') as file:
  
    # Reading the content of the file
    # using the read() function and storing
    # them in a new variable
    data = file.read()
  
    # Searching and replacing the text
    # using the replace() function
    data = data.replace('http', ' ')
  
# Opening our text file in write only
# mode to write the replaced content
with open(f"files_cleaned/mastodon_cleaned.txt", 'w') as file:
  
    # Writing the replaced data in our
    # text file
    file.write(data)

cleanupmastodon = open(f"files_cleaned/mastodon_cleaned.txt", 'r')
newmastodoncontent = cleanupmastodon.read()
print(newmastodoncontent)

"""Now based on the cleaned up file, WordCloud will be created to visualize what are the most common terms used in both policies. 

For the purpose of this paper, max words are limited to 30
"""

# # create Word Clouds

from wordcloud import WordCloud    
import matplotlib.pyplot as plt

wordcloud = WordCloud(max_font_size=40, collocations=False, max_words=30).generate(newtwittercontent)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

# save Word Clouds
! mkdir wordclouds
wordcloud.to_file(f"wordclouds/word_cloud_twitter.png")

# Establish lexical density
# text_clean_slice = text_clean [0:600]
# ld_results = len(set(text_clean_slice)) / len(text_clean_slice)
# print(ld_results)
# ld_dict = {'File_name': filepath, 'lexical_density': ld_results}
# lexical_density.append(ld_dict)

# print(lexical_density)

wordcloud2 = WordCloud(max_font_size=40, collocations=False, max_words=30).generate(newmastodoncontent)
plt.figure()
plt.imshow(wordcloud2, interpolation="bilinear")
plt.axis("off")
plt.show()

# save Word Clouds
wordcloud.to_file(f"wordclouds/word_cloud_mastodon.png")

"""Both Twitter and Mastodon has information as the most frequenty used terms. It makes sense given the privacy policy is about how the company addresses user's information.

Some other notable terms from Twitter is provide, share, ad, access, affiliate, partner, collect. 

There were many mentions of sharing the data as previously mentioned in the document. This can be due to how much data Twitter collects and use those to generate profit: through advertisement. 

For Mastodon, notable terms were server, deliver, IP, site, register, log.

This can be due to how Mastodon operates with distributed servers. 

What is most interesting from the wordcloud is that both Twitter and Matodon had the term 'may' in their top 30 most frequently used terms. The use of term 'may' can create confusion to the users as may in definition expresses possibility.

For instance, Twitter provides following statements:

“Professional Accounts. If you create a professional account, you also need to provide us with a professional category, and **may** provide us with other information, including street address, contact email address, and contact phone number, all of which will always be public."

Also

“3.4 With our Affiliates.
We **may** share information amongst our affiliates to provide our products and services.”

May could be interpreted as optional information that users could provide but it can also be interpreted as Twitter possibly sharing information with other parties. 

As for Mastodon, the policy provides as follows:

“Basic account information: If you register on this server, you **may** be asked to enter a username, an e- mail address and a password. You **may** also enter additional profile information such as a display name and biography, and upload a profile picture and header image. The username, display name, biography, profile picture and header image are always listed publicly.”

Unless there are alternative ways to register on the server, the statement should be more definitive than merely expressing possibility.

Both Twitter and Mastodon could provide clearer langagues surrounding how the data is collected and shared, if those data are certainly collected, used and shared.

## Lexical Density

[Lexical density](https://osawec.elc.cityu.edu.hk/repo/front-page/writing-tips/lexical-density/#:~:text=Assessed%20with%20Lexical%20Density%2C%20a,nouns%20while%20adverbs%20modify%20verbs) is used to find out the density of text or speech by dividing the number of lexical words by total number of words. Lexical words are verbs, nouns, adjectives and adverbs. According to [Languages Humanities](https://www.languagehumanities.org/what-is-lexical-density.htm),  a balanced lexicon density would be around 50% and academic papers, government documents and  jargon-filled documents tend to produce higher densities
"""

# Establish lexical density for Twitter
text_clean_slice = newtwittercontent [0:1000]
ld_results = len(set(text_clean_slice)) / len(text_clean_slice)

print(ld_results)

# Establish lexical density for Mastodon
text_clean_slice2 = newmastodoncontent [0:1000]
ld_results2 = len(set(text_clean_slice2)) / len(text_clean_slice2)
print(ld_results2)

"""Considering both Mastodon and Twitter are less than 50%, we can infer that the documents are not as jargon heavy and easy for users to read and digest.

## TF-IDF

TFIDF aims to identify the most distinctively frequent or significant words in a document. 

First, download necessary libraries
"""

! pip install sklearn

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
pd.options.display.max_rows = 600
from pathlib import Path  
import glob

"""Set Directory"""

directory_path = "files_cleaned/"
text_files = glob.glob(f"{directory_path}/*.txt")

text_files

text_titles = [Path(text).stem for text in text_files]

text_titles

"""Run TfidfVectorizer"""

tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')

tfidf_vector = tfidf_vectorizer.fit_transform(text_files)

type(tfidf_vector)

"""Turn the result into dataframe"""

tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())

tfidf_df = tfidf_df.stack().reset_index()

"""Rename Dataframe Columns"""

tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document','level_1': 'term'})

"""Print top 10 terms and highest tfidf scores"""

tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)

"""Visualize the TFIDF using altair"""

!pip install altair

import altair as alt
import numpy as np

# Terms in this list will get a red dot in the visualization
term_list = ['information', 'share']

# adding a little randomness to break ties in term ranking
top_tfidf_plusRand = top_tfidf.copy()
top_tfidf_plusRand['tfidf'] = top_tfidf_plusRand['tfidf'] + np.random.rand(top_tfidf.shape[0])*0.0001

# base for all visualizations, with rank calculation
base = alt.Chart(top_tfidf_plusRand).encode(
    x = 'rank:O',
    y = 'document:N'
).transform_window(
    rank = "rank()",
    sort = [alt.SortField("tfidf", order="descending")],
    groupby = ["document"],
)

# heatmap specification
heatmap = base.mark_rect().encode(
    color = 'tfidf:Q'
)

# red circle over terms in above list
circle = base.mark_circle(size=100).encode(
    color = alt.condition(
        alt.FieldOneOfPredicate(field='term', oneOf=term_list),
        alt.value('red'),
        alt.value('#FFFFFF00')        
    )
)

# text labels, white for darker heatmap colors
text = base.mark_text(baseline='middle').encode(
    text = 'term:N',
    color = alt.condition(alt.datum.tfidf >= 0.23, alt.value('white'), alt.value('black'))
)

# display the three superimposed visualizations
(heatmap + circle + text).properties(width = 600)

"""The result is similar to what was shown using WordCloud. 

Both Twitter and Mastodon has information as top 2 frequenty used term as this is an analysis of privacy policy. Considering the low TFIDF scores throughout the top 10 terms, it is difficult to conclude which policy has more depth, however, this shows how these two companies operate. Mastodon highlights its servers while Twitter highlights its service.

## Limitations
It should be noted that the main distinction between Twitter and Mastodon is the difference in servers. This means that Twitter has one server that collects and stores all data and is governed by Twitter’s own privacy policy, while Mastodon has several servers and the data are collected to these different servers, based on user’s choice and follow both [Mastodon and the servers’ own privacy policies](https://themarkup.org/the-breakdown/2022/11/21/we-joined-mastodon-heres-what-we-learned-about-privacy-and-security ). Considering Mastodon has more than [12,000 servers](https://mastodon.help/instances/en ), this research will only contain privacy policies of Mastodon social itself, not the servers.

##Replicability

Use the same codes for other platforms and policies as long as the text is scraped through the PDF version. It would be interesting to see how other platforms that are as mature as Twitter, such as facebook and youtube address in their policies and it would be interesting to look at other policies such as community guidelines or code of conduct to answer other research questions to understand the functionality of other sites.

## Conclusion

Twitter in fact is more comprehensive. Mastodon does not answer some of the baseline questions. Both can be done better in terms of using vague language. Similar to lexical density to provide easy to read policy document for users, Twitter’s document seems to focus on its services while Mastodon focuses on the servers.
"""